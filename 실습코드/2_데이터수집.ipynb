{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPSAEgGMfqbFqysXKlAv2fY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. 데이터 수집\n","- 기본적으로 데이터를 읽고, 핸들링하는데 사용하는 패키지는 `numpy`, `pandas`입니다.\n","- 패키지를 등록하기 위해서는 `import 패키지명 as 간축어`를 통해서 사용합니다.  "],"metadata":{"id":"9WWV7NFU-ioi"}},{"cell_type":"markdown","source":["## 1.1 데이터 파일 읽기"],"metadata":{"id":"j4PFBn855MF4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"g0w8z0vV-cUQ"},"outputs":[],"source":["# package 등록\n","import numpy as np\n","import pandas as pd"]},{"cell_type":"code","source":["# 데이터 읽기\n","sample = pd.read_csv(\"sample.csv\")\n","sample.head()\n","\n","# 엑셀 파일을 읽는 방법\n","# pd.read_excel(\"filename.xlsx\", sheet_name = \"Sheet1\", usecols = [\"A\", \"C\"])"],"metadata":{"id":"ClgQsrRtAEnA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 크기 확인\n","sample.shape"],"metadata":{"id":"Dgi0GOhJAz0x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터의 정보 확인\n","sample.info()\n","sample.describe()"],"metadata":{"id":"YHqHpZnUBSpX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 컬럼 선택\n","sample['date']"],"metadata":{"id":"nUBAUzSYBph3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 원하는 데이터만 일부 추출\n","sample[sample['death'] != 0]"],"metadata":{"id":"VCRpER3OBxwS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 원하는 데이터 행 선택\n","#sample[1:2]\n","#sample.iloc[2]\n","\n","# tail의 의미\n","#sample.tail()\n","#sample.iloc[-5:]\n","\n","# 원하는 열 선택\n","#sample.iloc[:, 0:2]\n","#sample[['date', 'cnt']]\n","\n","# 원하는 행 삭제\n","#sample.drop(0)\n","#sample.drop([0,1])\n","\n","# 원하는 열 삭제\n","#sample.drop('date', axis = 1)"],"metadata":{"id":"aV-ev2Z9-j1j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1.2. 크롤링을 이용한 데이터 수집 (BeautifulSoup이용)\n","- 데이터를 직접 다운로드 할 수 있겠지만, 데이터가 따로 업로드 되어있지 않은 경우, 웹 사이트에 있는 원하는 데이터를 추출하는 방법을 스크래핑 혹은 크롤링 이라고 합니다.\n","- `BeautifulSoup`은 파이썬에서 크롤링을 하기위해 사용하는 대표적인 라이브러리 중 하나입니다."],"metadata":{"id":"DPA0a3P55R8M"}},{"cell_type":"code","source":["import requests\n","from bs4 import BeautifulSoup\n","\n","# 1. 웹페이지 요청\n","url = \"https://news.naver.com/\"   # 네이버 뉴스 메인 페이지\n","response = requests.get(url)\n","\n","# 2. HTML 파싱\n","soup = BeautifulSoup(response.text, features=\"html.parser\")\n","\n","# 3. 원하는 데이터 추출 (기사 제목)\n","# strong 태그 중 class=\"cnf_news_title\"만 추출\n","titles = soup.find_all(\"strong\", class_=\"cnf_news_title\")\n","\n","print(\"=== 네이버 뉴스 기사 제목 ===\")\n","for t in titles[:10]:\n","    print(t.get_text(strip=True))"],"metadata":{"id":"4_8A_B5R5rIw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. 웹페이지 요청\n","url = \"https://news.naver.com/\"   # 네이버 뉴스 메인 페이지\n","response = requests.get(url)\n","print(response)\n","print(response.text)"],"metadata":{"id":"Qx6ImbB66_h7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. HTML 파싱\n","soup = BeautifulSoup(response.text, features=\"html.parser\")\n","soup"],"metadata":{"collapsed":true,"id":"J-7gEiaG7KC6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 3. 원하는 데이터 추출 (기사 제목)\n","# strong 태그 중 class=\"cnf_news_title\"만 추출\n","titles = soup.find_all(\"strong\", class_=\"cnf_news_title\")\n","print(titles[0])\n","print(\" \")\n","print(\" \")\n","print(\" \")\n","print(\"=== 네이버 뉴스 기사 제목 ===\")\n","# 10개 정도만 우선 불러오기\n","for t in titles[:10]:\n","    print(t.get_text(strip=True))"],"metadata":{"id":"Ehtzke2Y71Sc"},"execution_count":null,"outputs":[]}]}